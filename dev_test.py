import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import tadprep as tp  # Testing package-level import - I want TADPREP to mirror Pandas in its common practice

# FIRST-PASS TESTING
# Uploading sample datafile
df_raw = pd.read_csv(r"C:\Users\doncs\Documents\GitHub\TADPREP\data\sample_data_longlean.csv")

# Checking import
# print(df_raw)


# Testing file_info method
# tp.file_info(df_raw, verbose=False)
# tp.file_info(df_raw, verbose=True)


# Testing reshape method
# df_reshape = tp.reshape(df_raw, verbose=False)
# df_reshape = tp.reshape(df_raw, verbose=True)
# print(df_reshape)


# Testing rename_and_tag method
# df_renamed = tp.rename_and_tag(df_raw, verbose=False, tag_features=False)
# df_renamed = tp.rename_and_tag(df_raw, verbose=True, tag_features=False)
# df_renamed = tp.rename_and_tag(df_raw, verbose=False, tag_features=True)
# df_renamed = tp.rename_and_tag(df_raw, verbose=True, tag_features=True)
# print(df_renamed)


# Testing feature_stats method
# tp.feature_stats(df_raw, verbose=False, summary_stats=False)
# tp.feature_stats(df_raw, verbose=True, summary_stats=False)
# tp.feature_stats(df_raw, verbose=False, summary_stats=True)
# tp.feature_stats(df_raw, verbose=True, summary_stats=True)


# Testing impute method
# df_impute = tp.impute(df_raw, verbose=False, skip_warnings=False)
# df_impute = tp.impute(df_raw, verbose=True, skip_warnings=False)
# df_impute = tp.impute(df_raw, verbose=False, skip_warnings=True)
# df_impute = tp.impute(df_raw, verbose=True, skip_warnings=True)
# print(df_impute)


# Testing encode method
# Note that this method must be tested with and without a passed list of features to encode
# encode_feats = ['sex', 'race', 'us_citizen']

# df_encode = tp.encode(df_raw, encode_feats, verbose=True, skip_warnings=True)
# df_encode = tp.encode(df_raw, encode_feats, verbose=True, skip_warnings=False)
# df_encode = tp.encode(df_raw, encode_feats, verbose=False, skip_warnings=True)
# df_encode = tp.encode(df_raw, encode_feats, verbose=False, skip_warnings=False)

# df_encode = tp.encode(df_raw, verbose=True, skip_warnings=True)
# df_encode = tp.encode(df_raw, verbose=True, skip_warnings=False)
# df_encode = tp.encode(df_raw, verbose=False, skip_warnings=True)
# df_encode = tp.encode(df_raw, verbose=False, skip_warnings=False)
# print(df_encode)


# Testing scale method
# Note that this method must be tested with and without a passed list of features to scale
# scale_feats = ['age']

# df_scaled = tp.scale(df_raw, scale_feats, verbose=True, skip_warnings=True)
# df_scaled = tp.scale(df_raw, scale_feats, verbose=True, skip_warnings=False)
# df_scaled = tp.scale(df_raw, scale_feats, verbose=False, skip_warnings=True)
# df_scaled = tp.scale(df_raw, scale_feats, verbose=False, skip_warnings=False)

# df_scaled = tp.scale(df_raw, verbose=True, skip_warnings=True)
# df_scaled = tp.scale(df_raw, verbose=True, skip_warnings=False)
# df_scaled = tp.scale(df_raw, verbose=False, skip_warnings=True)
# df_scaled = tp.scale(df_raw, verbose=False, skip_warnings=False)
# print(df_scaled)


# Final test step is checking the full prep_df method
# Start with creating a sample dataframe of smaller scale
# df_test = pd.DataFrame({
#     'name': ['Jack', 'Joe', 'Jill', 'Jane', 'John'],
#     'age': [29, 31, 37, 34, 41],
#     'sex': ['M', 'M', 'F', 'F', np.nan],
#     'married': [0, 0, 1, 1, 1],
#     'happy': [3, 4, 5, 2, 2],
#     'region': ['Northeast', 'Central', 'Central', np.nan, np.nan]
# })

# Check dataframe content
# print(df_test)

# Define feature lists to pass at method call
# prep_encode = ['sex', 'married']
# prep_scale = ['age']

# Test without any passed feature lists
# processed_df = tp.prep_df(df_test)

# Test with one passed feature list
# processed_df = tp.prep_df(df_test, prep_encode)

# Test with two passed feature lists
# processed_df = tp.prep_df(df_test, prep_encode, prep_scale)

# Check prepared dataframe
# print(processed_df)


# SECOND-PASS TESTING (checking method augmentations with specific corner cases)

'''
Testing augmented df_info method
Need to check for detection of:
- Duplicate instances
- Low-variance features
- Infinite values
- Non-null empty strings
'''
info_df = pd.DataFrame({
    'name': ['John', 'Joe', 'Jack', 'James', 'Jake', 'John'],  # Creating one duplicate row for 'John'
    'age': [25, 57, 17, 31, np.nan, 25],  # Make sure we have a missing value
    'weight': [155, 155, 155, 155, 155, 155],  # Should trip the low-variance check
    'height': [67, 70, np.inf, 72, 65, 67],  # Should trip the infinite values check
    'hair_color': ['brown', 'black', '', 'blonde', 'bald', 'brown']  # Should trip the non-null empty string check
})

# This should report on the duplicated instance, but should not display the follow-up warning
# tp.df_info(info_df, verbose=False)

# This should display all warnings
# tp.df_info(info_df, verbose=True)


'''
Testing new subset method
Need to test for:
- True random sampling in subset
- Seeded random sampling in subset
- Stratified random sampling in subset
- Time-boundary creation in subsetting timeseries data
- Whether methodology explanations work as intended
- Whether automatic time interval identification works properly
- Summary of subset results
- Differentiation of UX in verbose vs. non-verbose modes 
'''
# Build non-timeseries dataset with values which should make subsetting validation easy
# subset_normal = pd.DataFrame(
#     {'id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
#      'name': ['Adam', 'Alice', 'Alex', 'Adrian', 'Ashton', 'Amanda', 'Alice', 'Ashley', 'Aaron', 'Austin'],
#      'sex': ['M', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'M'],
#      'age': [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]}
# )

# df_subset_random = tp.subset(subset_normal, verbose=True)  # Verbose true random subset
# df_subset_random = tp.subset(subset_normal, verbose=False)  # Minimal-output true random subset
# print(df_subset_random)  # Print check

# df_subset_seeded = tp.subset(subset_normal, verbose=True)  # Verbose seeded random subset
# print(df_subset_seeded)

# df_subset_strat = tp.subset(subset_normal, verbose=True)  # Verbose stratified subset
# df_subset_strat = tp.subset(subset_normal, verbose=False)  # Minimal-output stratified subset
# print(df_subset_strat)

# Testing at day level
# subset_ts = pd.DataFrame(
#     {'date': [(datetime.now() - timedelta(days=x)).strftime('%Y-%m-%d %H:%M') for x in range(10)],
#      'river_volume': [101513, 175474, 167819, 130656, 110106, 113368, 109843, 119749, 110872, 196164]}
# )
# # df_subset_ts = tp.subset(subset_ts, verbose=True)  # Verbose time-bound subset
# df_subset_ts = tp.subset(subset_ts, verbose=False)  # Minimal-output time-bound subset
# print(subset_ts)
# print()
# print(df_subset_ts)

# Testing at hour level
# subset_ts = pd.DataFrame(
#     {'date': [(datetime.now() - timedelta(hours=x)).strftime('%Y-%m-%d %H:%M') for x in range(10)],
#      'river_volume': [101513, 175474, 167819, 130656, 110106, 113368, 109843, 119749, 110872, 196164]}
# )
# df_subset_ts = tp.subset(subset_ts, verbose=True)  # Verbose time-bound subset
# print(subset_ts)
# print()
# print(df_subset_ts)


'''
Testing augmented rename_and_tag method
Need to test for:
- Detection of invalid python identifiers (e.g. new feature name that starts with a number)
- Detection of problematic characters (e.g. new feature name that contains a special character)
- Detection of bad anti-pattern (e.g. new feature which is all uppercase or very short)
- Detection of python keyword duplication (e.g. new feature name is the same as a python keyword)
- Whether the method catches duplicate feature names
- Whether the method catches "All selected features already tagged as ordinal"
- Whether the method catches "All selected features already tagged as targets"
- Format and appearance of change summary table
- Whether the verbose and non-verbose UXs are functioning appropriately 
'''
# df_orig = pd.DataFrame({
#     'animal': ['Elephant', 'Rhino', 'Hippo', 'Cheetah', 'Lion'],
#     'size_ord': [3, 2, 2, 1, 1],
#     'eats_people_target': ['No', 'No', 'No', 'Yes', 'Yes']
# })
# print(df_orig)  # Print check
# df_rename = tp.rename_and_tag(df_orig, verbose=True, tag_features=False)  # Test verbose with no tagging
# df_rename = tp.rename_and_tag(df_orig, verbose=False, tag_features=False)  # Test minimal output with no tagging
# df_rename = tp.rename_and_tag(df_orig, verbose=True, tag_features=True)  # Test verbose with tagging
# df_rename = tp.rename_and_tag(df_orig, verbose=False, tag_features=True)  # Test minimal output with tagging
# print(df_rename)


'''
Testing augmented impute method
Need to test for:
- Detection of false-numerical features
- Detection of low-variance features
- Detection of high outlier counts
- Detection of highly-correlated feature pairs
- Detection of timeseries data
- Detection of high missingness rate
- Constant-value imputation
- Random-sample imputation
- Visualizations (both single-feature and before-after)
- Final imputation summary table
'''
# non_ts_data = {
#     # Numerical features with different characteristics
#     'normal': [100, 102, 98, 103, 97, 101, np.nan, 100, 102, 98, 100, np.nan, 101, 97, 103],
#     'skewed': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, np.nan, 1.0, 1.0, 2.0, 5.0, 15.0, 30.0, 50.0, 100.0],  # Extreme skew
#     'low_var': [50.01, 50.02, 50.01, 50.02, 50.02, 50.01, 50.02, 50.01, np.nan, 50.02, 50.01, 50.02, 50.01, 50.02,
#                 50.01],
#     'outliers': [105, 102, 98, 350, 101, 103, 99, 345, 101, np.nan, 100, 355, 102, 98, 101],
#     'corr_1': [2, 4, 6, 8, 10, 12, 14, 16, np.nan, 20, 22, 24, 26, 28, 30],
#     'corr_2': [2.1, 4.2, 6.1, 8.1, 10.2, 12.1, 14.2, 16.1, np.nan, 20.2, 22.1, 24.2, 26.1, 28.2, 30.1],
#
#     # Categorical feature
#     'cat_normal': ['A', 'B', np.nan, 'B', 'B', 'A', 'B', 'A', np.nan, 'B', 'A', 'B', np.nan, 'B', np.nan, ],
#
#     # False numerical features (actually categorical or ordinal)
#     'false_num_bin': [0, np.nan, 0, 1, 1, 0, 1, 0, 1, np.nan, 0, 1, 0, 1, 0],
#     'false_num_ord': [np.nan, 2, 3, 2, np.nan, 1, 4, 2, 3, 1, np.nan, 2, 4, 3, np.nan]
# }
#
# # Timeseries test data
# ts_data = {
#     'date': [(datetime.now() - timedelta(days=x)).strftime('%Y-%m-%d') for x in range(15)],
#     'numeric_value': [100, np.nan, 98, 102, np.nan, 99, 101, 98, 102, np.nan, 105, 99, 101, np.nan, 103],
#     'categorical': ['A', 'B', np.nan, 'A', 'B', 'A', np.nan, 'B', 'A', 'B', np.nan, 'A', 'B', 'A', 'B']
# }

# Create test DataFrames
# df_normal = pd.DataFrame(non_ts_data)
# df_ts = pd.DataFrame(ts_data).sort_values('date').reset_index(drop=True)

# Test impute method on non-timeseries data
# df_normal_imputed = tp.impute(df_normal, verbose=True, skip_warnings=False)  # Full walkthrough
# df_normal_imputed = tp.impute(df_normal, verbose=False, skip_warnings=True)  # Super-streamlined
# print(df_normal_imputed)

# df_ts_imputed = tp.impute(df_ts, verbose=True, skip_warnings=False)  # Full walkthrough
# df_ts_imputed = tp.impute(df_ts, verbose=False, skip_warnings=True)  # Super-streamlined
# print(df_ts_imputed)


'''
Testing augmented feature_stats method
Need to test for:
- Fixed distribution pattern calculation for categorical features (verifying top N value coverage percentages)
- Empty datetime feature handling (when all values are null or can't be converted)
- Consistent numeric formatting across all statistics
  - Large integers (should show commas)
  - Decimal values (should display 4 decimal places)
  - Integer-like floats (should display as integers without decimals)
- Skewness metric testing:
  - Approximately symmetric distributions (|skew| < 0.5)
  - Moderately skewed distributions (0.5 ≤ |skew| < 1)
  - Highly skewed distributions (|skew| ≥ 1)
- Kurtosis metric testing:
  - Platykurtic distributions (kurt < -0.5)
  - Mesokurtic distributions (-0.5 ≤ kurt ≤ 0.5)
  - Leptokurtic distributions (kurt > 0.5)
- Edge cases:
  - Features with NaN values
  - Single-value features
  - Features with extremely large/small values
- Different numerical distributions:
  - Symmetric vs. skewed distributions
  - Features with zero mean (for coefficient of variation)
'''
# Create a test dataframe
# df_feature_stats = pd.DataFrame({
#     # Boolean features - both formats
#     'bool_true_false': [True, True, False, True, False, True, True, False, True, False, True, False, True, True, None,
#                         True, False, True, False, True],
#     'bool_binary': [1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1],
#
#     # Datetime feature - explicit datetime
#     'date_explicit': [
#         datetime(2023, 1, 1),
#         datetime(2023, 1, 2),
#         datetime(2023, 1, 3),
#         datetime(2023, 1, 4),
#         datetime(2023, 1, 5),
#         datetime(2023, 1, 6),
#         None,
#         datetime(2023, 1, 8),
#         datetime(2023, 1, 9),
#         datetime(2023, 1, 10),
#         datetime(2023, 1, 11),
#         datetime(2023, 1, 12),
#         datetime(2023, 1, 13),
#         datetime(2023, 1, 14),
#         datetime(2023, 1, 15),
#         datetime(2023, 1, 16),
#         datetime(2023, 1, 17),
#         datetime(2023, 1, 18),
#         datetime(2023, 1, 19),
#         datetime(2023, 1, 20)
#     ],
#
#     # Datetime as strings - to test detection logic
#     'date_strings': ['01-01-2023', '02-01-2023', '03-01-2023', '04-01-2023', '05-01-2023', '06-01-2023', None, None,
#                      '10-01-2023', '11-01-2023', None, '01-01-2024', '02-01-2024', '03-01-2024', '04-01-2024', None,
#                      '06-01-2024', '07-01-2024', '08-01-2024', '09-01-2024'],
#
#     # Categorical features
#     'cat_normal': ['A', 'B', 'C', 'A', 'B', 'D', 'A', 'E', 'B', 'A', 'C', 'D', 'E', 'B', None, 'A', 'C', 'B', 'A', 'D'],
#     'cat_empty': ['A', 'B', 'C', '', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', '', 'N', 'O', 'P', 'Q', 'R'],
#
#     # Zero-variance feature - to test detection
#     'zero_variance': ['Same'] * 20,
#
#     # Near-constant feature (exactly 95% single value)
#     'near_constant': ['Frequent'] * 19 + ['Rare'],  # 19/20 = 95%
#
#     # Duplicated feature - to test potential duplicate detection
#     'cat_normal_dup': ['A', 'B', 'C', 'A', 'B', 'D', 'A', 'E', 'B', 'A', 'C', 'D', 'E', 'B', None, 'A', 'C', 'B',
#                        'A', 'D'],
#
#     # Numerical features
#     # Large integers - for formatting with commas
#     'num_large': [1000000, 2000000, 3000000, 4000000, 5000000, 6000000, 7000000, 8000000, 9000000, 10000000, 11000000,
#                   12000000, 13000000, 14000000, 15000000, 16000000, 17000000, 18000000, 19000000, 20000000],
#
#     # Decimal values - for 4 decimal place formatting
#     'num_decimal': [10.1234, 11.5678, 9.7531, 10.5, 12.6789, 8.25, 11.125, 9.5, 10.375, None, 13.4567, 7.8901, 12.3456,
#                     9.9999, 11.1111, 14.7890, 8.6543, 10.0001, 12.5000, 9.8765],
#
#     # Integer-like floats - should display as integers
#     'num_int_floats': [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0,
#                        18.0, 19.0, 20.0],
#
#     # Skewness test cases
#     # Approximately symmetric (|skew| < 0.5)
#     'num_symmetric': [50, 45, 55, 48, 52, 47, 53, 49, 51, 50, 46, 54, 48, 52, 50, 47, 53, 49, 51, 50],
#
#     # Moderately skewed (0.5 ≤ |skew| < 1)
#     'num_mod_skewed': [10, 12, 11, 13, 14, 18, 12, 11, 10, 15, 13, 12, 14, 16, 11, 17, 13, 12, 14, 11],
#
#     # Highly skewed (|skew| ≥ 1)
#     'num_highly_skewed': [1, 2, 1, 3, 1, 2, 1, 10, 200, 500, 1, 2, 3, 1, 2, 1, 2, 3, 1, 100],
#
#     # Kurtosis test cases
#     # Platykurtic (kurt < -0.5)
#     'num_platykurtic': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 15, 25, 35, 45, 55, 65, 75, 85, 95, 5],
#
#     # Mesokurtic (-0.5 ≤ kurt ≤ 0.5)
#     'num_mesokurtic': [100, 95, 105, 98, 102, 97, 103, 99, 101, 100, 96, 104, 98, 102, 100, 97, 103, 99, 101, 100],
#
#     # Leptokurtic (kurt > 0.5)
#     'num_leptokurtic': [50, 50, 50, 50, 50, 51, 49, 10, 90, 50, 50, 50, 50, 49, 51, 50, 50, 50, 50, 50],
#
#     # Zero mean - for coefficient of variation handling
#     'num_zero_mean': [-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
#
#     # Feature with nulls
#     'num_with_nulls': [1, 2, None, 4, 5, None, 7, 8, 9, 10, None, 12, 13, 14, 15, None, 17, 18, 19, 20]})
#
# # tp.feature_stats(df_feature_stats, verbose=True)  # Testing full output
# tp.feature_stats(df_feature_stats, verbose=False)  # Testing minimal output

'''
Testing augmented scale method
Need to test for:
- Operation with and without features_to_scale list
- Preservation of original features
- Detection of false-numeric features (e.g. 0/1 'Yes'/'No' format)
- Detection of null values, infinite values, high skewness
- Before/after visualization
- Custom minmax scaler ranges
'''

'''
Testing refactored encode method
Need to test for:
1. Core Functionality:
   - Basic one-hot and dummy encoding operations
   - Operation with and without features_to_encode list
   - Clean column name generation for encoded features
2. New Features & Fixes:
   - Preservation of original features (preserve_features=True)
   - Handling of all-NaN columns (should be skipped)
   - Proper reindexing when using missing_strat='drop' with non-continuous indices
   - Memory efficiency with large datasets (performance test)
3. Missing Value Handling:
   - 'ignore' strategy (default) - leaves NaNs in encoded columns
   - 'category' strategy - creates separate indicator column for NaNs
   - 'drop' strategy - processes only non-null values
4. Edge Cases:
   - Detection of false-numeric features (e.g., 0/1 'Yes'/'No' format)
   - Handling of high-cardinality features
   - Handling columns with single value (should be skipped)
   - Handling columns with rare categories
   - Safely handling values that can't be converted to float
5. User Interface:
   - Custom reference category selection for dummy encoding
   - Custom prefix selection for encoded columns
   - Before/after distribution visualizations
   - Proper warning display and suppression
'''